{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKPHLyJ+FDMwpIvqoNg+Pa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Darshan0805/Deep-Learning/blob/main/Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Assume we are attempting to train a back-propagation neural network with two hidden layers and a single node in each layer. Let us assume that the activation function is sigmoid function and all the weights and biases are initially set to w=1, b= -0.5, respectively. Compute the forward-pass output for input 0.5. Let the input 0.5 correspond to the target output y =1. Demonstrate the process of learning using Gradient Descent at for 2 iterations to obtain the target 1 for the input 0.5.        "
      ],
      "metadata": {
        "id": "QKmD44wBjwFj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4pi6aP1jpGp",
        "outputId": "20e107ef-6633-4372-b766-7ba4090bab2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: Output = 0.5, Error = 0.125\n",
            "Iteration 2: Output = 0.5039061705290805, Error = 0.12401588788463572\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Initialize the network parameters\n",
        "x = 0.5\n",
        "y_target = 1\n",
        "w_hidden1 = w_hidden2 = w_output = 1\n",
        "b_hidden1 = b_hidden2 = b_output = -0.5\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Define the sigmoid activation function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# Perform forward pass and backward pass for 2 iterations\n",
        "for i in range(2):\n",
        "    # Forward pass\n",
        "    z_hidden1 = w_hidden1 * x + b_hidden1\n",
        "    a_hidden1 = sigmoid(z_hidden1)\n",
        "\n",
        "    z_hidden2 = w_hidden2 * a_hidden1 + b_hidden2\n",
        "    a_hidden2 = sigmoid(z_hidden2)\n",
        "\n",
        "    z_output = w_output * a_hidden2 + b_output\n",
        "    a_output = sigmoid(z_output)\n",
        "\n",
        "    # Compute the error for the output layer\n",
        "    error_output = (y_target - a_output) * sigmoid_derivative(z_output)\n",
        "\n",
        "    # Update the weights and biases for the output layer\n",
        "    w_output += learning_rate * error_output * a_hidden2\n",
        "    b_output += learning_rate * error_output\n",
        "\n",
        "    # Print the output and error for each iteration\n",
        "    print(f\"Iteration {i+1}: Output = {a_output}, Error = {error_output}\")\n"
      ]
    }
  ]
}